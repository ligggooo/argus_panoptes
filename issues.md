# jiliang_monitor

*集成组后台运维管理系统*

1. QueuePool达到最大连接数问题
    可能是链接未及时释放导致
    也可能是postgres或者sql alchemy设置不当导致
        updated 2021/3/10:
            1）session 用完之后及时关闭 用expunge或者freeze来保存信息
            2）使用缓存机制减少对数据库的直接访问
        updated 2021/3/29:
            1) 定时器访问唤醒。完成
        updated 2021/4/5
            1) 配置连接池释放时间，使之小于postgres的连接保持时间
2. 偶尔会出现记录丢失
    丢失之后怎么容错
    如何降低丢失的几率
        updated 2021/3/7:
            1）日志丢失导致树的割裂暂时无解
            2）日志入库顺序打乱引起的树构造错误已经通过增加代码解决
            3）装饰器标错（cross_process标在normal上）导致树节点错挂，暂时无解
3. task的前端呈现需要更清晰
    任务起始时间+树形展开
        updated 2021/3/10:
            1）起始时间统计需要后端merge records时增加一项
            2）树形表示需要使用element UI的树形控件或者e-charts的树图支持
4. 节点记录状态融合和最终呈现信息之间的处理过程混乱
    有必要进行一次重构，将信息加工过程的逻辑梳理清楚
    前端显示需要重新设计，将状态+错误+辅助信息以不同的颜色分栏列出
5. 缺一个pull镜像的功能
        updated 2021/3/31:
            1）前端没有这个按钮
6. 缺批量部署的前端接口，目前在本地手动脚本操作完成
7. 容器管理页面需要依次访问docker服务器，响应非常慢
8. 节点任务起始时间问题
        updated 2021/4/7:
            1）目前初始化为-1
9. 后端返回数据量过大，前端处理缓慢
        updated 2021/4/7:
            1）一次取出一个节点的子节点，在有4000个子任务的情形下，依然很慢
10. 搜索支持： id搜索，时间范围搜索，关键词搜索
11. 记录字段添加环境变量，保证子任务的物理环境可被跟踪
12. 任务树构建在第一次加载时非常缓慢 树节点查找算法需要优化
        updated 2021/4/11:
            先前的查找方式是基于调用关系树进行的，即使考虑了子节点编号一定比父节点大这一
            先验信息，现有的查询方式的复杂度也基本是介于二分查找和线性查找之间的。
            暂时改成哈希表查找。时间复杂度降下来了。
        updated 2021/4/11 12:36:
            发现随着子任务记录的增加，监控器响应时间逐渐变长。
            2w个子任务时，增加记录的请求响应时间已经逐渐从100ms增加到了460ms。
            说明树结构的操作已经占据了请求处理的大部分时间，而且随节点增加，请求响应速度是越来越慢的。
            这种情况下，若还要保持树缓存在每一个进程中都有副本，开销会更大。
            有必要优化树结构体更新的逻辑了。
            思路一：不再实时更新树结构了，或者至少异步进行。 因为前端的请求并不频繁，后端不需要一直实时刷新；
            思路二：更多地将树结构题的操作交给hash表来完成。（没想清楚还）
        updated 2021/4/14:
            经过数据库异步写入的优化 + 缓存结构的调整 现在达到了30 并发，20000任务 平均响应时间 0.18 qps 165
            树状态更新的时机变更到了查询接口中。
13. 多线程服务模式下，任务执行数目统计经常变化--可能有线程竞争现象
        updated 2021/4/14:
            加了很多锁。
14. 大量任务压测时，监控后端响应时间达到10秒量级
        updated 2021/4/14:
            经过数据库异步写入的优化 + 缓存结构的调整 现在达到了30 并发，20000任务 平均响应时间 0.18
            qps 165
15. 服务器被多进程启动时，无法维持全局id的唯一性
        updated 2021/4/11:
            借用redis实现了全局唯一自增id。借用uwsgi实现了flask服务器的高性能部署。
        updated 2021/4/11:
            现在的问题是： flask工作在多进程服务模型下，之前的缓存方案无法使用了！！
        updated 2021/4/14:
            经过优化，性能达到预期，暂时不必往多进程方向优化了。
16. 基于哈希表的缓存方案无法为工作在多进程模式下的flask服务提供缓存。 todo
        除非使用观察者模式，数据直接插入redis。
        每个服务器进程都订阅之，redis变化，服务器变化。
17. 描述信息不应该过分限制长度：1024字节太少。 # todo
        考虑拆分成多条记录或者分表实现 # todo
18. 心跳检测 # todo
19. 解决黄绿闪烁问题。
20. 打日志记录请求开始和结束的时间，便于后端性能分析 # todo